diff --git a/config.yaml b/config.yaml
index 39ca048..3eca471 100644
--- a/config.yaml
+++ b/config.yaml
@@ -32,7 +32,7 @@ modeling:
     min_samples_leaf: 3
     # Here -1 means all available cores
     n_jobs: -1
-    criterion: mae
+    criterion: absolute_error
     max_features: 0.5
     # DO not change the following
     oob_score: true
diff --git a/environment.yml b/environment.yml
index 6cfa2cf..21a2568 100644
--- a/environment.yml
+++ b/environment.yml
@@ -11,6 +11,7 @@ dependencies:
   - hydra-core=1.3.2
   - matplotlib=3.8.0
   - pandas=1.3.5
+  - pandas-profiling=3.1.0 
   - git=2.42.0
   - pip=23.3.1
   - pip:
diff --git a/main.py b/main.py
index 0d2c490..79cc6c2 100644
--- a/main.py
+++ b/main.py
@@ -97,14 +97,21 @@ def go(config: DictConfig):
             with open(rf_config, "w+") as fp:
                 json.dump(dict(config["modeling"]["random_forest"].items()), fp)  # DO NOT TOUCH
 
-            # NOTE: use the rf_config we just created as the rf_config parameter for the train_random_forest
-            # step
-
-            ##################
-            # Implement here #
-            ##################
-
-            pass
+            # model training
+            _ = mlflow.run(
+                os.path.join(root_path, 
+                             "src", 
+                             "train_random_forest"),
+                "main",
+                parameters={
+                    "trainval_artifact": "trainval_data.csv:latest",
+                    "val_size": config['modeling']['val_size'],
+                    "stratify_by": config['modeling']['stratify_by'],
+                    "rf_config": rf_config,
+                    "max_tfidf_features": config['modeling']['max_tfidf_features'],
+                    "output_artifact": "random_forest_export",
+                },
+            )
 
         if "test_regression_model" in active_steps:
 
diff --git a/src/eda/EDA.ipynb b/src/eda/EDA.ipynb
index 5dc31ca..e3d9c53 100644
--- a/src/eda/EDA.ipynb
+++ b/src/eda/EDA.ipynb
@@ -29,7 +29,7 @@
     {
      "data": {
       "text/html": [
-       "Run data is saved locally in <code>/Users/saurabhbhardwaj/Documents/rental-prices-nyc-MLPipeline/src/eda/wandb/run-20231101_224619-zlixnfqs</code>"
+       "Run data is saved locally in <code>/Users/saurabhbhardwaj/Documents/rental-prices-nyc-MLPipeline/src/eda/wandb/run-20231102_113909-ixy72s1x</code>"
       ],
       "text/plain": [
        "<IPython.core.display.HTML object>"
@@ -41,7 +41,7 @@
     {
      "data": {
       "text/html": [
-       "Syncing run <strong><a href='https://wandb.ai/aryansaurabhbhardwaj/nyc_airbnb/runs/zlixnfqs' target=\"_blank\">exalted-blaze-13</a></strong> to <a href='https://wandb.ai/aryansaurabhbhardwaj/nyc_airbnb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
+       "Syncing run <strong><a href='https://wandb.ai/aryansaurabhbhardwaj/nyc_airbnb/runs/ixy72s1x' target=\"_blank\">fine-bird-35</a></strong> to <a href='https://wandb.ai/aryansaurabhbhardwaj/nyc_airbnb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
       ],
       "text/plain": [
        "<IPython.core.display.HTML object>"
@@ -65,7 +65,7 @@
     {
      "data": {
       "text/html": [
-       " View run at <a href='https://wandb.ai/aryansaurabhbhardwaj/nyc_airbnb/runs/zlixnfqs' target=\"_blank\">https://wandb.ai/aryansaurabhbhardwaj/nyc_airbnb/runs/zlixnfqs</a>"
+       " View run at <a href='https://wandb.ai/aryansaurabhbhardwaj/nyc_airbnb/runs/ixy72s1x' target=\"_blank\">https://wandb.ai/aryansaurabhbhardwaj/nyc_airbnb/runs/ixy72s1x</a>"
       ],
       "text/plain": [
        "<IPython.core.display.HTML object>"
@@ -78,7 +78,7 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x11287e200>> (for post_run_cell), with arguments args (<ExecutionResult object at 11287fa00, execution_count=1 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 11291ebc0, raw_cell=\" import wandb\n",
+      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x103b7e440>> (for post_run_cell), with arguments args (<ExecutionResult object at 103b7f310, execution_count=1 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 103b7ed70, raw_cell=\" import wandb\n",
       " import pandas as pd\n",
       "\n",
       " run = wandb.i..\" store_history=True silent=False shell_futures=True cell_id=54b1294a-a341-404d-a44e-c1714862560a> result=None>,),kwargs {}:\n"
diff --git a/src/train_random_forest/run.py b/src/train_random_forest/run.py
index d8f37d4..f9d4c40 100644
--- a/src/train_random_forest/run.py
+++ b/src/train_random_forest/run.py
@@ -9,6 +9,7 @@ import shutil
 import matplotlib.pyplot as plt
 
 import mlflow
+from mlflow.models import infer_signature
 import json
 
 import pandas as pd
@@ -51,11 +52,8 @@ def go(args):
     # Fix the random seed for the Random Forest, so we get reproducible results
     rf_config['random_state'] = args.random_seed
 
-    ######################################
-    # Use run.use_artifact(...).file() to get the train and validation artifact (args.trainval_artifact)
-    # and save the returned path in train_local_pat
-    trainval_local_path = # YOUR CODE HERE
-    ######################################
+    logger.info("Downloading train-validation artifact")
+    trainval_local_path = run.use_artifact(args.trainval_artifact).file()       
 
     X = pd.read_csv(trainval_local_path)
     y = X.pop("price")  # this removes the column "price" from X and puts it into y
@@ -67,16 +65,11 @@ def go(args):
     )
 
     logger.info("Preparing sklearn pipeline")
-
     sk_pipe, processed_features = get_inference_pipeline(rf_config, args.max_tfidf_features)
 
     # Then fit it to the X_train, y_train data
-    logger.info("Fitting")
-
-    ######################################
-    # Fit the pipeline sk_pipe by calling the .fit method on X_train and y_train
-    # YOUR CODE HERE
-    ######################################
+    logger.info("Fitting the pipeline")
+    sk_pipe.fit(X_train, y_train)
 
     # Compute r2 and MAE
     logger.info("Scoring")
@@ -88,36 +81,41 @@ def go(args):
     logger.info(f"Score: {r_squared}")
     logger.info(f"MAE: {mae}")
 
-    logger.info("Exporting model")
-
+    logger.info("Exporting the model")
     # Save model package in the MLFlow sklearn format
     if os.path.exists("random_forest_dir"):
         shutil.rmtree("random_forest_dir")
 
-    ######################################
-    # Save the sk_pipe pipeline as a mlflow.sklearn model in the directory "random_forest_dir"
-    # HINT: use mlflow.sklearn.save_model
-    # YOUR CODE HERE
-    ######################################
+    signature = infer_signature(X_val[processed_features], y_pred)
 
-    ######################################
-    # Upload the model we just exported to W&B
-    # HINT: use wandb.Artifact to create an artifact. Use args.output_artifact as artifact name, "model_export" as
-    # type, provide a description and add rf_config as metadata. Then, use the .add_dir method of the artifact instance
-    # you just created to add the "random_forest_dir" directory to the artifact, and finally use
-    # run.log_artifact to log the artifact to the run
-    # YOUR CODE HERE
-    ######################################
+    path = "random_forest_dir"
+
+    mlflow.sklearn.save_model(
+        sk_pipe, path, 
+        serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_CLOUDPICKLE, 
+        signature=signature,
+        input_example=X_val.iloc[:2]) 
+
+    artifact = wandb.Artifact(
+        name=args.output_artifact,
+        type="model_export",
+        description="Random Forest model export",
+        metadata=rf_config
+        )
+    
+    artifact.add_dir(path)   
+
+    logger.info("Logging model artifacts")
+    run.log_artifact(artifact)  
 
     # Plot feature importance
     fig_feat_imp = plot_feature_importance(sk_pipe, processed_features)
 
     ######################################
-    # Here we save r_squared under the "r2" key
+    # save r_squared under the "r2" key
     run.summary['r2'] = r_squared
-    # Now log the variable "mae" under the key "mae".
-    # YOUR CODE HERE
-    ######################################
+    # save r_squared under the "mae" key
+    run.summary['mae'] = mae
 
     # Upload to W&B the feture importance visualization
     run.log(
@@ -158,7 +156,9 @@ def get_inference_pipeline(rf_config, max_tfidf_features):
     # Build a pipeline with two steps:
     # 1 - A SimpleImputer(strategy="most_frequent") to impute missing values
     # 2 - A OneHotEncoder() step to encode the variable
-    non_ordinal_categorical_preproc = # YOUR CODE HERE
+    non_ordinal_categorical_preproc = make_pipeline(
+        SimpleImputer(strategy="most_frequent"), OneHotEncoder()
+    )
     ######################################
 
     # Let's impute the numerical columns to make sure we can handle missing values
@@ -214,10 +214,12 @@ def get_inference_pipeline(rf_config, max_tfidf_features):
 
     ######################################
     # Create the inference pipeline. The pipeline must have 2 steps: a step called "preprocessor" applying the
-    # ColumnTransformer instance that we saved in the `preprocessor` variable, and a step called "random_forest"
-    # with the random forest instance that we just saved in the `random_forest` variable.
-    # HINT: Use the explicit Pipeline constructor so you can assign the names to the steps, do not use make_pipeline
-    sk_pipe = # YOUR CODE HERE
+    sk_pipe = Pipeline(
+        steps=[
+            ("preprocessor", preprocessor),
+            ("random_forest", random_Forest),
+        ]
+    )
 
     return sk_pipe, processed_features
 
